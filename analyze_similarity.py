import os
import re
from pathlib import Path
import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import streamlit as st
from typing import List, Dict

# ============== CONFIG ==============
SIMILARITY_THRESHOLDS = {
    'critical': 0.90,
    'warning': 0.75,
    'info': 0.60
}

# ============== PARSER MARKDOWN ==============
def parse_markdown_brief(file_path: str) -> List[Dict]:
    """Parse markdown brief do struktury z heading/knowledge/keywords"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    article_name = Path(file_path).stem
    sections = []
    
    # Split content by ## headings (any level - ##, ###, ####)
    # Pattern: finds ## X\. Heading (markdown files have literal backslash before dot)
    parts = re.split(r'\n(##+ \d+(?:\\\.\d+)*\\..*?)\n', content)
    
    # Process pairs: heading + content
    for i in range(1, len(parts), 2):
        if i+1 >= len(parts):
            break
            
        heading_raw = parts[i].strip()
        section_content = parts[i+1]
        
        # Extract section number from heading (e.g., "## 1\." -> "1" or "### 4\." -> "4")
        section_num_match = re.search(r'##+ (\d+(?:\\\.\d+)*)', heading_raw)
        if not section_num_match:
            continue
        section_num = section_num_match.group(1)

        # Clean heading (remove ##, number, and backslash-dot)
        heading_clean = re.sub(r'^##+ \d+(?:\\\.\d+)*\\\.\s*', '', heading_raw)
        heading_clean = re.sub(r'[\*\_]', '', heading_clean).strip()
        
        # Extract Wiedza (from **Wiedza:** to **Keywords:** or **SÅ‚owa kluczowe:**)
        # Note: Content may start with \n, and there may be spaces after colons
        knowledge_match = re.search(r'\*\*Wiedza:\*\*\s+\n(.*?)\n\*\*(?:Keywords|SÅ‚owa kluczowe):\*\*', section_content, re.DOTALL)
        if not knowledge_match:
            # Try alternative format without extra spaces
            knowledge_match = re.search(r'\*\*Wiedza:\*\*\s*\n(.*?)\n\*\*(?:Keywords|SÅ‚owa kluczowe):\*\*', section_content, re.DOTALL)
        knowledge = knowledge_match.group(1).strip() if knowledge_match else ""

        # Extract Keywords (from **Keywords:** or **SÅ‚owa kluczowe:** to end or next ##)
        keywords_match = re.search(r'\*\*(?:Keywords|SÅ‚owa kluczowe):\*\*\s+\n(.*?)(?=\n##|$)', section_content, re.DOTALL)
        if not keywords_match:
            # Try alternative format
            keywords_match = re.search(r'\*\*(?:Keywords|SÅ‚owa kluczowe):\*\*\s*\n(.*?)(?=\n##|$)', section_content, re.DOTALL)
        keywords = keywords_match.group(1).strip() if keywords_match else ""
        
        if knowledge:  # Only add if we found knowledge
            sections.append({
                'id': f"{article_name}_sec{section_num}",
                'article': article_name,
                'section_num': section_num,
                'heading': heading_clean,
                'knowledge': knowledge,
                'keywords': keywords,
                'combined_text': f"{heading_clean}\n\n{knowledge}\n\nKeywords: {keywords}"
            })
    
    return sections
# ============== MAIN ANALYSIS ==============
@st.cache_resource
def load_model():
    return SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

def analyze_similarities(all_sections: List[Dict], model, threshold: float = 0.60):
    """Analyze similarities between sections"""
    
    # Generate embeddings
    progress_bar = st.progress(0, text="ğŸ§  Generowanie embeddingÃ³w...")
    
    texts = [s['combined_text'] for s in all_sections]
    embeddings = model.encode(texts, show_progress_bar=False)
    
    progress_bar.progress(50, text="ğŸ” Obliczanie podobieÅ„stw...")
    
    # Calculate similarity matrix
    similarity_matrix = cosine_similarity(embeddings)
    
    progress_bar.progress(75, text="ğŸ“Š Wyszukiwanie par...")
    
    # Find pairs above threshold
    pairs = []
    n = len(all_sections)
    
    for i in range(n):
        for j in range(i+1, n):
            sim = similarity_matrix[i][j]
            
            if sim >= threshold:
                pairs.append({
                    'section_1': all_sections[i],
                    'section_2': all_sections[j],
                    'similarity': sim,
                    'same_article': all_sections[i]['article'] == all_sections[j]['article']
                })
    
    # Sort by similarity
    pairs.sort(key=lambda x: x['similarity'], reverse=True)
    
    progress_bar.progress(100, text="âœ… Gotowe!")
    
    return pairs, similarity_matrix

# ============== STREAMLIT UI ==============
def main():
    st.set_page_config(page_title="Content Similarity Analyzer", layout="wide")
    
    st.title("ğŸ“Š Content Similarity Analyzer")
    st.markdown("Analiza podobieÅ„stwa sekcji briefÃ³w - detekcja kanibalizacji treÅ›ci")
    
    # Sidebar - Config
    st.sidebar.header("âš™ï¸ Konfiguracja")
    
    folder_path = st.sidebar.text_input("ÅšcieÅ¼ka do folderu z briefami:", value="./briefy")
    threshold = st.sidebar.slider("PrÃ³g podobieÅ„stwa (INFO):", 0.0, 1.0, 0.60, 0.05)
    
    show_same_article = st.sidebar.checkbox("PokaÅ¼ podobieÅ„stwa w tym samym artykule", value=True)
    show_diff_article = st.sidebar.checkbox("PokaÅ¼ podobieÅ„stwa miÄ™dzy artykuÅ‚ami", value=True)
    
    if st.sidebar.button("ğŸš€ URUCHOM ANALIZÄ˜", type="primary"):
        
        # Load files
        st.header("ğŸ“‚ Åadowanie plikÃ³w...")
        
        folder = Path(folder_path)
        if not folder.exists():
            st.error(f"âŒ Folder nie istnieje: {folder_path}")
            return
            
        md_files = list(folder.glob("*.md"))
        
        if not md_files:
            st.error(f"âŒ Nie znaleziono plikÃ³w .md w folderze: {folder_path}")
            return
        
        st.success(f"âœ… Znaleziono {len(md_files)} plikÃ³w")
        
        # Parse files
        all_sections = []
        with st.expander("ğŸ“„ Przetwarzanie plikÃ³w", expanded=True):
            for md_file in md_files:
                try:
                    sections = parse_markdown_brief(md_file)
                    all_sections.extend(sections)
                    
                    if sections:
                        st.write(f"âœ… {md_file.name}: **{len(sections)}** sekcji")
                    else:
                        st.warning(f"âš ï¸ {md_file.name}: 0 sekcji (sprawdÅº format)")
                        
                except Exception as e:
                    st.error(f"âŒ BÅ‚Ä…d w {md_file.name}: {e}")
        
        if not all_sections:
            st.error("âŒ Nie zaÅ‚adowano Å¼adnych sekcji! SprawdÅº format plikÃ³w.")
            
            with st.expander("ğŸ” Debug - PrzykÅ‚ad oczekiwanego formatu"):
                st.code("""
## 1. NagÅ‚Ã³wek pierwszej sekcji
**Wiedza:**
TreÅ›Ä‡ wiedzy dla pierwszej sekcji...

**SÅ‚owa kluczowe:**
sÅ‚owo1, sÅ‚owo2, sÅ‚owo3

## 2. NagÅ‚Ã³wek drugiej sekcji
**Wiedza:**
TreÅ›Ä‡ wiedzy dla drugiej sekcji...

**SÅ‚owa kluczowe:**
sÅ‚owo4, sÅ‚owo5, sÅ‚owo6
                """, language="markdown")
            return
        
        st.success(f"ğŸ“Š **ÅÄ…cznie: {len(all_sections)} sekcji** z {len(set([s['article'] for s in all_sections]))} artykuÅ‚Ã³w")
        
        # Load model and analyze
        with st.spinner("Åadowanie modelu embeddingowego..."):
            model = load_model()
        
        pairs, similarity_matrix = analyze_similarities(all_sections, model, threshold)
        
        # Filter pairs based on checkboxes
        filtered_pairs = []
        for pair in pairs:
            if pair['same_article'] and show_same_article:
                filtered_pairs.append(pair)
            elif not pair['same_article'] and show_diff_article:
                filtered_pairs.append(pair)
        
        # Statistics
        st.header("ğŸ“ˆ Statystyki")
        
        col1, col2, col3, col4 = st.columns(4)
        
        critical = sum(1 for p in filtered_pairs if p['similarity'] >= 0.90)
        warning = sum(1 for p in filtered_pairs if 0.75 <= p['similarity'] < 0.90)
        info = sum(1 for p in filtered_pairs if 0.60 <= p['similarity'] < 0.75)
        
        col1.metric("ğŸ”´ CRITICAL (â‰¥90%)", critical)
        col2.metric("ğŸŸ¡ WARNING (75-90%)", warning)
        col3.metric("ğŸ”µ INFO (60-75%)", info)
        col4.metric("ğŸ“ Razem", len(filtered_pairs))
        
        # Display pairs
        st.header("ğŸ” Wykryte podobieÅ„stwa")
        
        if not filtered_pairs:
            st.success("âœ… Brak podobieÅ„stw powyÅ¼ej progu!")
            return
        
        # Filter options
        severity_filter = st.selectbox(
            "Filtruj po poziomie:",
            ["Wszystkie", "ğŸ”´ Critical", "ğŸŸ¡ Warning", "ğŸ”µ Info"]
        )
        
        displayed_count = 0
        
        for idx, pair in enumerate(filtered_pairs):
            sim = pair['similarity']
            
            # Determine severity
            if sim >= 0.90:
                badge = "ğŸ”´ CRITICAL"
                color = "#e74c3c"
            elif sim >= 0.75:
                badge = "ğŸŸ¡ WARNING"
                color = "#f39c12"
            else:
                badge = "ğŸ”µ INFO"
                color = "#3498db"
            
            # Apply filter
            if severity_filter == "ğŸ”´ Critical" and sim < 0.90:
                continue
            elif severity_filter == "ğŸŸ¡ Warning" and (sim < 0.75 or sim >= 0.90):
                continue
            elif severity_filter == "ğŸ”µ Info" and (sim < 0.60 or sim >= 0.75):
                continue
            
            displayed_count += 1
            
            # Display pair
            with st.container():
                st.markdown(f"### Para #{displayed_count}")
                
                col_badge, col_sim, col_status = st.columns([2, 2, 3])
                
                with col_badge:
                    st.markdown(f"<h3 style='color:{color};'>{badge}</h3>", unsafe_allow_html=True)
                with col_sim:
                    st.markdown(f"**PodobieÅ„stwo: {sim:.1%}**")
                with col_status:
                    if pair['same_article']:
                        st.success("âœ“ Ten sam artykuÅ‚")
                    else:
                        st.error("âš  RÃ³Å¼ne artykuÅ‚y")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown(f"**ğŸ“„ ArtykuÅ‚ 1**")
                    st.info(f"**{pair['section_1']['article']}**")
                    st.markdown(f"**Sekcja:** {pair['section_1']['section_num']}")
                    st.markdown(f"**NagÅ‚Ã³wek:** {pair['section_1']['heading']}")
                    
                    with st.expander("ğŸ“– Wiedza", expanded=False):
                        st.text_area("", pair['section_1']['knowledge'], height=200, key=f"knowledge1_{idx}", disabled=True)
                    
                    with st.expander("ğŸ·ï¸ Keywords", expanded=False):
                        st.text(pair['section_1']['keywords'])
                
                with col2:
                    st.markdown(f"**ğŸ“„ ArtykuÅ‚ 2**")
                    st.info(f"**{pair['section_2']['article']}**")
                    st.markdown(f"**Sekcja:** {pair['section_2']['section_num']}")
                    st.markdown(f"**NagÅ‚Ã³wek:** {pair['section_2']['heading']}")
                    
                    with st.expander("ğŸ“– Wiedza", expanded=False):
                        st.text_area("", pair['section_2']['knowledge'], height=200, key=f"knowledge2_{idx}", disabled=True)
                    
                    with st.expander("ğŸ·ï¸ Keywords", expanded=False):
                        st.text(pair['section_2']['keywords'])
                
                st.divider()
        
        # Export to CSV
        st.header("ğŸ’¾ Export")

        # Prepare CSV data
        df_export = []
        for pair in filtered_pairs:
            df_export.append({
                'PodobieÅ„stwo': f"{pair['similarity']:.3f}",
                'ArtykuÅ‚ 1': pair['section_1']['article'],
                'Sekcja 1': pair['section_1']['section_num'],
                'NagÅ‚Ã³wek 1': pair['section_1']['heading'],
                'ArtykuÅ‚ 2': pair['section_2']['article'],
                'Sekcja 2': pair['section_2']['section_num'],
                'NagÅ‚Ã³wek 2': pair['section_2']['heading'],
                'Ten sam artykuÅ‚': 'TAK' if pair['same_article'] else 'NIE'
            })

        df = pd.DataFrame(df_export)
        csv = df.to_csv(index=False, encoding='utf-8-sig')

        # Direct download button
        st.download_button(
            label="ğŸ“¥ Pobierz raport CSV",
            data=csv,
            file_name="similarity_report.csv",
            mime="text/csv",
            use_container_width=False
        )

if __name__ == "__main__":
    main()